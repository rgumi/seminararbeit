{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seminarpaper",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rgumi/seminararbeit/blob/master/seminarpaper-xgb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rrQrTCrhbO8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pre processing\n",
        "from scipy.stats import randint, uniform\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "import datetime as dt\n",
        "from sklearn.preprocessing import MinMaxScaler, OrdinalEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "#import xgboost as xgb\n",
        "#from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_validate, RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, make_scorer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAW5xaxrnqxF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# setup\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/saschaschworm/big-data-and-data-science/master/datasets/prediction-challenge/dataset.csv',\n",
        "                   index_col='identifier', parse_dates=['date'])\n",
        "# shows all unique values for each column\n",
        "#for (name, data) in data.iteritems():\n",
        "#  print(name, data.unique())\n",
        "\n",
        "X, y = data.iloc[:, 1:-1], data['success']\n",
        "\n",
        "hyperparams = { 'random_state': 1909\n",
        "}\n",
        "model = RandomForestClassifier(**hyperparams)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9mxt-2Uk_5V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop not needed features to reduce complexity and overfitting\n",
        "# X.drop(['communication_type'], axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WydWwqETnep3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pipeline\n",
        "categorical_features = ['marital_status', 'education', 'job', 'credit_default', 'housing_loan', 'personal_loan', 'communication_type', 'previous_conversion']\n",
        "numeric_features = ['age', 'n_contacts_campaign', 'days_since_last_contact', 'n_contacts_before']\n",
        "\n",
        "numeric_transformer = Pipeline([\n",
        "    ('scaler', MinMaxScaler()),\n",
        "])\n",
        "categorical_transformer = Pipeline ([\n",
        "    ('onehotencoder', OrdinalEncoder())\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('n_transformer', numeric_transformer, numeric_features),\n",
        "    ('c_transformer', categorical_transformer, categorical_features),\n",
        "])\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', model)\n",
        "])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8h4MCjsnXBN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "0f219a38-6e98-415a-a56e-2011a91490eb"
      },
      "source": [
        "# randomized hyperparameter search\n",
        "custom_scorer = make_scorer(f1_score, pos_label='Yes')\n",
        "n_estimators = randint(100, 600)\n",
        "max_depth = randint(50, 400)\n",
        "\n",
        "param_distributions = { 'model__n_estimators': n_estimators, \n",
        "                        'model__max_depth': max_depth\n",
        "}\n",
        "\n",
        "rs = RandomizedSearchCV(pipeline, param_distributions=param_distributions, n_iter=5,\n",
        "                       scoring=custom_scorer, n_jobs=-1, iid=False, cv=10, random_state=1909)\n",
        "\n",
        "rs = rs.fit(X, y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGEibMYUnRQy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run optimized model\n",
        "hyperparams = {'n_estimators': rs.best_params_['model__n_estimators'], \n",
        "               'criterion': 'gini', \n",
        "               'max_depth': rs.best_params_['model__max_depth'],\n",
        "               'min_samples_split': 2,\n",
        "               'min_samples_leaf': 1, 'min_weight_fraction_leaf': 0.0, 'max_features': 'auto',\n",
        "               'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None,\n",
        "               'bootstrap': True, 'oob_score': False, 'n_jobs': None, 'random_state': 1909,\n",
        "               'verbose': 0, 'warm_start': False, 'class_weight': None\n",
        "               }\n",
        "\n",
        "\n",
        "model = RandomForestClassifier(**hyperparams)\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor), \n",
        "    ('model', model)\n",
        "])\n",
        "pipeline.fit(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRYFhgewnOFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# validating\n",
        "res_cv = cross_validate(pipeline, X, y, scoring=custom_scorer, cv=10, return_train_score=True)\n",
        "res_f1_tr = np.mean(res_cv['train_score']) * 100\n",
        "res_f1_te = np.mean(res_cv['test_score']) * 100\n",
        "print(hyperparams)\n",
        "result = f'Average F1 on Training and Test Sets: {res_f1_tr:.2f}%/{res_f1_te:.2f}%'\n",
        "print(result)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4tOSNqMdCvF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# persists results for later analysis\n",
        "import json\n",
        "from datetime import datetime as dt\n",
        "\n",
        "mountpath = \"/content/drive\"\n",
        "from google.colab import drive\n",
        "drive.mount(mountpath)\n",
        "\n",
        "resultJSON = {\n",
        "    \"date\": dt.now().strftime(\"%d.%m.%Y, %H:%M:%S\"),\n",
        "    \"result\": result,\n",
        "    \"hyperparams\": hyperparams\n",
        "}\n",
        "\n",
        "with open(mountpath + '/My Drive/seminararbeit/results.txt', 'a') as file:\n",
        "  file.write(str(resultJSON) + '\\n')\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}