{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final-v1.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rgumi/seminararbeit/blob/master/final_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIo03i7VErog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.stats import randint, uniform\n",
        "import datetime as dt\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import json\n",
        "import urllib.request\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_validate, cross_val_score\n",
        "from sklearn.model_selection import RandomizedSearchCV, RepeatedStratifiedKFold, GridSearchCV\n",
        "from sklearn.metrics import f1_score, make_scorer, confusion_matrix, accuracy_score, classification_report\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from collections import Counter\n",
        "from xgboost.sklearn import XGBClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65g0sBlAx75d",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8UHa8-ZyA22",
        "colab_type": "text"
      },
      "source": [
        "## Feature Additions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjf8i_m7vd3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_euribor = pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/rgumi/seminararbeit_src/master/refined/euribor3m_ref.csv', \n",
        "    index_col=['index'], parse_dates=['date'])\n",
        "\n",
        "df_euribor = df_euribor[(df_euribor['date'].dt.year >= 2007)]\n",
        "\n",
        "def get_euribor(date):\n",
        "  for i in range(0, len(df_euribor)):\n",
        "\n",
        "    if date >= df_euribor['date'].iloc[i]:\n",
        "      last = df_euribor['value'].iloc[i]\n",
        "      continue\n",
        "    return round(last, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a8uQAdvOVte",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_eurostoxx= pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/rgumi/seminararbeit_src/master/refined/eurostoxx_ref.csv', \n",
        "    index_col=['index'], parse_dates=['date'])\n",
        "\n",
        "df_eurostoxx = df_eurostoxx[(df_eurostoxx['date'].dt.year >= 2007)]\n",
        "\n",
        "def get_eurostoxx(date):\n",
        "  for i in range(0, len(df_eurostoxx)):\n",
        "\n",
        "    if date >= df_eurostoxx['date'].iloc[i]:\n",
        "      last = df_eurostoxx['value'].iloc[i]\n",
        "      continue\n",
        "    return last"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JS4n-_a4OgdJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://www.financialresearch.gov/financial-stress-index/\n",
        "df_fsi= pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/rgumi/seminararbeit_src/master/refined/fsi_ref.csv', \n",
        "    index_col=['index'], parse_dates=['Date'])\n",
        "\n",
        "df_fsi = df_fsi[(df_fsi['Date'].dt.year >= 2007)]\n",
        "\n",
        "def get_fsi(date):\n",
        "  for i in range(0, len(df_fsi)):\n",
        "\n",
        "    if date >= df_fsi['Date'].iloc[i]:\n",
        "      # Possible values: [OFR FSI, Credit, Equity valuation, Safe assets, Funding, Volatility]\n",
        "      last = df_fsi['OFR FSI'].iloc[i]\n",
        "      continue\n",
        "    return last"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33cyR0R7Llxb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_cpi = pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/rgumi/seminararbeit_src/master/refined/cpi_monthly_ref.csv', \n",
        "    index_col=['index'], parse_dates=['date'])\n",
        "\n",
        "df_cpi = df_cpi[(df_cpi['date'].dt.year >= 2007)]\n",
        "\n",
        "def get_cpi(date):\n",
        "  for i in range(0, len(df_cpi)):\n",
        "\n",
        "    if date >= df_cpi['date'].iloc[i]:\n",
        "      last = df_cpi['value'].iloc[i]\n",
        "      continue\n",
        "    return last"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgxhChjBLUCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_cci = pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/rgumi/seminararbeit_src/master/refined/cci_monthly_ref.csv', \n",
        "    index_col=['index'], parse_dates=['date'])\n",
        "\n",
        "df_cci = df_cci[(df_cci['date'].dt.year >= 2007)]\n",
        "\n",
        "def get_cci(date):\n",
        "  for i in range(0, len(df_cci)):\n",
        "\n",
        "    if date >= df_cci['date'].iloc[i]:\n",
        "      last = df_cci['value'].iloc[i]\n",
        "      continue\n",
        "      \n",
        "    return last"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYAngATPuYD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ltz = {}\n",
        "with urllib.request.urlopen(\"https://raw.githubusercontent.com/rgumi/seminararbeit_src/master/refined/leitzinsen_eu.json\") as url:\n",
        "    tmp_ltz = json.loads(url.read().decode())\n",
        "for key in tmp_ltz.keys():\n",
        "  ltz[dt.datetime.strptime(key, '%d-%m-%Y')] = tmp_ltz[key]\n",
        "sorted_ltz = {k: ltz[k] for k in sorted(ltz)}\n",
        "\n",
        "def get_leitzins(date):\n",
        "  for key, val in sorted_ltz.items():\n",
        "    if date >= key:\n",
        "      last = val\n",
        "      continue\n",
        "    return last"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0IQ5mkTyTBt",
        "colab_type": "text"
      },
      "source": [
        "# Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8rRxxjwEm-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/saschaschworm/big-data-and-data-science/master/datasets/prediction-challenge/dataset.csv', \n",
        "    index_col='identifier', parse_dates=['date'])\n",
        "\n",
        "dataset.insert(len(dataset.columns) -1, \"weekday\", dataset.date.dt.weekday)\n",
        "dataset.insert(len(dataset.columns) -1, \"day\", dataset.date.dt.day)\n",
        "dataset.insert(len(dataset.columns) -1, \"month\", dataset.date.dt.month)\n",
        "dataset.insert(len(dataset.columns) -1, \"year\", dataset.date.dt.year)\n",
        "dataset.insert(len(dataset.columns) -1, \"quarter\", dataset.date.dt.quarter)\n",
        "\n",
        "# deleted cci (not useful)\n",
        "dataset.insert(len(dataset.columns)-1, \"leitzins\", dataset['date'].apply(get_leitzins))\n",
        "# dataset.insert(len(dataset.columns)-1, \"euribor\", dataset['date'].apply(get_euribor))\n",
        "dataset.insert(len(dataset.columns)-1, \"cpi\", dataset['date'].apply(get_cpi))\n",
        "dataset.insert(len(dataset.columns)-1, \"fsi\", dataset['date'].apply(get_fsi))\n",
        "dataset.insert(len(dataset.columns)-1, \"eurostoxx\", dataset['date'].apply(get_eurostoxx))\n",
        "\n",
        "dataset = dataset.drop('date', axis=1)\n",
        "\n",
        "dataset.loc[dataset['days_since_last_contact'] == -1, 'days_since_last_contact'] = 10000\n",
        "\n",
        "dataset.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chldv7-yEP04",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_features = ['day' ,'quarter', 'education', 'job', 'age', \n",
        "             'previous_conversion', 'n_contacts_before',\n",
        "             'days_since_last_contact', 'n_contacts_campaign',\n",
        "             'marital_status', 'credit_default', 'duration',\n",
        "             'housing_loan', 'personal_loan', 'leitzins',\n",
        "             'eurostoxx', 'fsi', 'cpi']\n",
        "\n",
        "categorical_features = ['education', 'job', 'previous_conversion',\n",
        "                        'marital_status', 'quarter', 'credit_default',\n",
        "                        'personal_loan', 'housing_loan']\n",
        "\n",
        "numerical_features = ['n_contacts_campaign', 'duration', \n",
        "                  'days_since_last_contact', 'age', 'cpi',\n",
        "                  'leitzins', 'eurostoxx', 'fsi', 'day']\n",
        "\n",
        "\n",
        "X = dataset[all_features]\n",
        "y = dataset['success']\n",
        "y = y.apply(lambda x: 1 if x == \"Yes\" else 0)\n",
        "\n",
        "# --- \n",
        "\n",
        "for item in categorical_features:\n",
        "  try:\n",
        "    encoded = pd.get_dummies(X[item], prefix=item)\n",
        "    X.drop(item, axis=1, inplace=True)\n",
        "    X = X.join(encoded)\n",
        "  except Exception as e:\n",
        "    print(\"Something went wrong?!\")\n",
        "    print(e)\n",
        "    continue\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X[numerical_features] = scaler.fit_transform(X[numerical_features])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFlNcu4m2foR",
        "colab_type": "text"
      },
      "source": [
        "# Model Selection\n",
        "\n",
        "The target variable of this problem is bianary (Yes/No). Therefore, we the model needs to be able to handle binary classification. The following models were selected for preselection:\n",
        "## Logistic Regression\n",
        "\n",
        "## Decision Tree (Random Forest)\n",
        "\n",
        "## Extreme Gradiant Boosting Classification (XGBClassifier)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuU0JXcs2faK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "custom_scorer = make_scorer(f1_score, pos_label=1)\n",
        "\n",
        "# The evaluation is done by running all models with the same preprocessed datasets\n",
        "# Every model will use their default hyperparameters. No optimization has been conducted\n",
        "# The evaluation is done with the F1 Score for all models using \n",
        "# a k-fold cross-validation (RepeatedStratifiedKFold)\n",
        "\n",
        "# function which takes a model and the training data to run a \n",
        "# cross_validate with the f1-scorer\n",
        "def validate_model(model, X, y):\n",
        "  scoring=custom_scorer\n",
        "  cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, \n",
        "                               random_state=1909)\n",
        "  scores = cross_validate(model, X, y, scoring=scoring, cv=cv, \n",
        "                          n_jobs=-1, return_train_score=True)\n",
        "  train_score = np.mean(scores[\"train_score\"])*100\n",
        "  test_score = np.mean(scores[\"test_score\"])*100\n",
        "  print(f'Mean F1 Score of {str(model).split(\"(\")[0]}: {train_score:.2f}/{test_score:.2f}\\n')\n",
        "  return {\n",
        "      'train': train_score,\n",
        "      'test': test_score\n",
        "      }\n",
        "\n",
        "model = LogisticRegression() # default hyperparameters\n",
        "print(f'Evaluating Model: {str(model).split(\"(\")[0]}')\n",
        "lgr_score = validate_model(model, X, y)\n",
        "\n",
        "model = RandomForestClassifier() # else: default hyperparameters\n",
        "print(model)\n",
        "print(f'Evaluating Model: {str(model).split(\"(\")[0]}')\n",
        "rfc_score = validate_model(model, X, y)\n",
        "\n",
        "model = XGBClassifier() # else: default hyperparameters\n",
        "print(model)\n",
        "print(f'Evaluating Model: {str(model).split(\"(\")[0]}')\n",
        "xgbc_score = validate_model(model, X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P7d4DFsXiyB",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameter optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Qi7nwcigeyl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "custom_scorer = make_scorer(f1_score, pos_label=1)\n",
        "\n",
        "def gridsearch(model, param_grid, X, y):\n",
        "  '''\n",
        "  Takes a model, param_grid and datasets \n",
        "  to perform a grid search\n",
        "  '''\n",
        "  search = GridSearchCV(model, param_grid=param_grid, scoring=custom_scorer,\n",
        "                    n_jobs=-1, cv=4, refit=True, error_score=0).fit(X, y)\n",
        "\n",
        "  print(f'Optimal parameters: {search.best_params_}')\n",
        "  print(f'Best Score: {search.best_score_}')\n",
        "  return search.best_params_\n",
        "\n",
        "def randsearch(model, param_distr, X, y):\n",
        "  '''\n",
        "  Takes a model, param_distribution and datasets \n",
        "  to perform a randomized search\n",
        "  '''\n",
        "  search = RandomizedSearchCV(model, param_distributions=param_distr, \n",
        "                              n_iter=10, scoring=custom_scorer, n_jobs=-1, \n",
        "                              cv=10, random_state=1909).fit(X, y)\n",
        "\n",
        "  print(f'Optimal parameters: {search.best_params_}')\n",
        "  print(f'Best Score: {search.best_score_}')\n",
        "  return search.best_params_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPKfRKY2I3OE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# basis hyperparams\n",
        "hyperparams = {\n",
        "    'scale_pos_weight': 9, # = sum(negative instances) / sum(positive instances) for imbalanced datasets\n",
        "    'gamma': 0, # minimum loss reduction needed to split a leaf node \n",
        "    'booster': 'gbtree', # => Decision Tree\n",
        "    'objective': 'binary:logistic', # Outputs the Probability computed using a logistic function \n",
        "    'silent': True, \n",
        "    'missing': None,\n",
        "    'nthread': -1,\n",
        "    'random_state': 1909, # => Reproducibility\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgpktDV1hUpo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.stats import randint, uniform\n",
        "\n",
        "custom_scorer = make_scorer(f1_score, pos_label=1)\n",
        "\n",
        "n_estimators = randint(100, 150) # default 100\n",
        "max_depth = randint(8, 50) # default 6\n",
        "gamma = randint(0, 5) # default 0\n",
        "learning_rate = uniform(0.005, 0.3) # default 0.3\n",
        "colsample_bynode = uniform(0.25, 1) # default 1\n",
        "colsample_bylevel = uniform(0.25, 1) # default 1\n",
        "colsample_bytree = uniform(0.25, 1) # default 1\n",
        "subsample = uniform(0.25, 1) # default 1\n",
        "base_score = uniform(0.2, 0.8) # default 0.5\n",
        "min_child_weight = uniform(0.2, 1) # default 1\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': n_estimators, # how many decision trees should be built\n",
        "    'max_depth': max_depth, # how many levels are allowed in a tree\n",
        "    'gamma': gamma, # minimum loss reduction needed to further split a leaf node \n",
        "    'learning_rate': learning_rate, # eta of the gradient decent (step size)\n",
        "    'subsample': subsample, # % of randomly sampled training data\n",
        "    'colsample_bynode': colsample_bynode, # % of randomly sampled features per node\n",
        "    'colsample_bylevel': colsample_bylevel, # % of randomly sampled features per level\n",
        "    'colsample_bytree': colsample_bytree, # % of randomly sampled features per tree\n",
        "    'base_score': base_score, # the base score every feature has at the start\n",
        "    'min_child_weight': min_child_weight, # minimum needed sum of weights to split a leaf node\n",
        "}\n",
        "\n",
        "best_params = randsearch(XGBClassifier(**hyperparams), \n",
        "                         param_distr=param_grid, X=X, y=y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkwIIGQLlgM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# with best parameters from optimization\n",
        "f_hyperparams = {**hyperparams, **{\n",
        "    'learning_rate': best_params['learning_rate'],\n",
        "    'n_estimators': best_params['n_estimators'],\n",
        "    'max_depth': best_params['max_depth'],\n",
        "    'colsample_bylevel': best_params['colsample_bylevel'],\n",
        "    'colsample_bynode': best_params['colsample_bynode'],\n",
        "    'colsample_bytree': best_params['colsample_bytree'],\n",
        "    'subsample': best_params['subsample'],\n",
        "    'base_score': best_params['base_score'],\n",
        "    'min_child_weight': best_params['min_child_weight'],\n",
        "    }\n",
        "}\n",
        "\n",
        "model = XGBClassifier(**f_hyperparams)\n",
        "print(model)\n",
        "validate_model(model, X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM7mwuyO8O1u",
        "colab_type": "text"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43J6FNEL8OUf",
        "colab_type": "code",
        "outputId": "2ab38ef4-0608-44d9-836f-cc49f68a6216",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "prediction_dataset = pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/saschaschworm/big-data-and-data-science/master/datasets/prediction-challenge/prediction-dataset.csv', \n",
        "    index_col='identifier', parse_dates=['date'])\n",
        "\n",
        "prediction_dataset.insert(len(prediction_dataset.columns) -1, \"weekday\", prediction_dataset.date.dt.weekday)\n",
        "prediction_dataset.insert(len(prediction_dataset.columns) -1, \"day\", prediction_dataset.date.dt.day)\n",
        "prediction_dataset.insert(len(prediction_dataset.columns) -1, \"month\", prediction_dataset.date.dt.month)\n",
        "prediction_dataset.insert(len(prediction_dataset.columns) -1, \"year\", prediction_dataset.date.dt.year)\n",
        "prediction_dataset.insert(len(prediction_dataset.columns) -1, \"quarter\", prediction_dataset.date.dt.quarter)\n",
        "\n",
        "prediction_dataset.insert(len(prediction_dataset.columns)-1, \"leitzins\", prediction_dataset['date'].apply(get_leitzins))\n",
        "#prediction_dataset.insert(len(prediction_dataset.columns)-1, \"euribor\", prediction_dataset['date'].apply(get_euribor))\n",
        "prediction_dataset.insert(len(prediction_dataset.columns)-1, \"cpi\", prediction_dataset['date'].apply(get_cpi))\n",
        "prediction_dataset.insert(len(prediction_dataset.columns)-1, \"fsi\", prediction_dataset['date'].apply(get_fsi))\n",
        "prediction_dataset.insert(len(prediction_dataset.columns)-1, \"eurostoxx\", prediction_dataset['date'].apply(get_eurostoxx))\n",
        "\n",
        "prediction_dataset = prediction_dataset.drop('date', axis=1)\n",
        "prediction_dataset.columns"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['age', 'marital_status', 'education', 'job', 'credit_default',\n",
              "       'housing_loan', 'personal_loan', 'communication_type',\n",
              "       'n_contacts_campaign', 'days_since_last_contact', 'n_contacts_before',\n",
              "       'previous_conversion', 'weekday', 'day', 'month', 'year', 'quarter',\n",
              "       'leitzins', 'euribor', 'cpi', 'fsi', 'eurostoxx', 'duration'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiUOM5Im8MiO",
        "colab_type": "code",
        "outputId": "e083c9ab-93fc-49d2-9022-018ca3308965",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_pred = prediction_dataset[all_features]\n",
        "\n",
        "for item in categorical_features:\n",
        "  try:\n",
        "    encoded = pd.get_dummies(X_pred[item], prefix=item)\n",
        "    X_pred.drop(item, axis=1, inplace=True)\n",
        "    X_pred = X_pred.join(encoded)\n",
        "  except Exception as e:\n",
        "    print(\"Something went wrong?!\")\n",
        "    print(e)\n",
        "    continue\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_pred[numerical_features] = scaler.fit_transform(X_pred[numerical_features])\n",
        "\n",
        "# uff... Add missing features to the prediction dataset at the position it has\n",
        "# in the training dataset\n",
        "for feature in X.columns:\n",
        "  if not feature in X_pred.columns:\n",
        "    print(f' X_pred is missing {feature}')\n",
        "    X_pred.insert(X.columns.tolist().index(feature), feature, 0)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " X_pred is missing credit_default_Yes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsSPk1JUJzOp",
        "colab_type": "code",
        "outputId": "b6516489-ed70-4b04-8842-6d549a1e441c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "mountpath = \"/content/drive\"\n",
        "from google.colab import drive\n",
        "drive.mount(mountpath)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzbQ05JN8KOx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = XGBClassifier(**f_hyperparams)\n",
        "model.fit(X, y)\n",
        "predictions = model.predict(X_pred)\n",
        "\n",
        "submission = pd.DataFrame(\n",
        "    predictions, index=X_pred.index, columns=['prediction'])\n",
        "\n",
        "matriculation_number = '465527'\n",
        "\n",
        "submission.to_csv(\n",
        "    f'{mountpath}/My Drive/seminararbeit/result/submission-{matriculation_number}.csv', index_label='identifier')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}