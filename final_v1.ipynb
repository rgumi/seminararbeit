{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final-v1.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rgumi/seminararbeit/blob/master/final_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIo03i7VErog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.stats import randint, uniform\n",
        "import datetime as dt\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import json\n",
        "import urllib.request\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_validate, cross_val_score\n",
        "from sklearn.model_selection import RandomizedSearchCV, RepeatedStratifiedKFold, GridSearchCV\n",
        "from sklearn.metrics import f1_score, make_scorer, confusion_matrix, accuracy_score, classification_report\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from xgboost.sklearn import XGBClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65g0sBlAx75d",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8UHa8-ZyA22",
        "colab_type": "text"
      },
      "source": [
        "## Feature Additions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjf8i_m7vd3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_euribor = pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/rgumi/seminararbeit_src/master/refined/euribor3m_ref.csv', \n",
        "    index_col=['index'], parse_dates=['date'])\n",
        "\n",
        "df_euribor = df_euribor[(df_euribor['date'].dt.year >= 2007)]\n",
        "\n",
        "def get_euribor(date):\n",
        "  for i in range(0, len(df_euribor)):\n",
        "\n",
        "    if date >= df_euribor['date'].iloc[i]:\n",
        "      last = df_euribor['value'].iloc[i]\n",
        "      continue\n",
        "    return round(last, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a8uQAdvOVte",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_eurostoxx= pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/rgumi/seminararbeit_src/master/refined/eurostoxx_ref.csv', \n",
        "    index_col=['index'], parse_dates=['date'])\n",
        "\n",
        "df_eurostoxx = df_eurostoxx[(df_eurostoxx['date'].dt.year >= 2007)]\n",
        "\n",
        "def get_eurostoxx(date):\n",
        "  for i in range(0, len(df_eurostoxx)):\n",
        "\n",
        "    if date >= df_eurostoxx['date'].iloc[i]:\n",
        "      last = df_eurostoxx['value'].iloc[i]\n",
        "      continue\n",
        "    return last"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JS4n-_a4OgdJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_fsi= pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/rgumi/seminararbeit_src/master/refined/fsi_ref.csv', \n",
        "    index_col=['index'], parse_dates=['Date'])\n",
        "\n",
        "df_fsi = df_fsi[(df_fsi['Date'].dt.year >= 2007)]\n",
        "\n",
        "def get_fsi(date):\n",
        "  for i in range(0, len(df_fsi)):\n",
        "\n",
        "    if date >= df_fsi['Date'].iloc[i]:\n",
        "      # Possible values: [OFR FSI, Credit, Equity valuation, Safe assets, Funding, Volatility]\n",
        "      last = df_fsi['OFR FSI'].iloc[i]\n",
        "      continue\n",
        "    return last"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33cyR0R7Llxb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_cpi = pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/rgumi/seminararbeit_src/master/refined/cpi_monthly_ref.csv', \n",
        "    index_col=['index'], parse_dates=['date'])\n",
        "\n",
        "df_cpi = df_cpi[(df_cpi['date'].dt.year >= 2007)]\n",
        "\n",
        "def get_cpi(date):\n",
        "  for i in range(0, len(df_cpi)):\n",
        "\n",
        "    if date >= df_cpi['date'].iloc[i]:\n",
        "      last = df_cpi['value'].iloc[i]\n",
        "      continue\n",
        "    return last"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgxhChjBLUCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_cci = pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/rgumi/seminararbeit_src/master/refined/cci_monthly_ref.csv', \n",
        "    index_col=['index'], parse_dates=['date'])\n",
        "\n",
        "df_cci = df_cci[(df_cci['date'].dt.year >= 2007)]\n",
        "\n",
        "def get_cci(date):\n",
        "  for i in range(0, len(df_cci)):\n",
        "\n",
        "    if date >= df_cci['date'].iloc[i]:\n",
        "      last = df_cci['value'].iloc[i]\n",
        "      continue\n",
        "      \n",
        "    return last"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYAngATPuYD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ltz = {}\n",
        "with urllib.request.urlopen(\"https://raw.githubusercontent.com/rgumi/seminararbeit_src/master/refined/leitzinsen_eu.json\") as url:\n",
        "    tmp_ltz = json.loads(url.read().decode())\n",
        "for key in tmp_ltz.keys():\n",
        "  ltz[dt.datetime.strptime(key, '%d-%m-%Y')] = tmp_ltz[key]\n",
        "sorted_ltz = {k: ltz[k] for k in sorted(ltz)}\n",
        "\n",
        "def get_leitzins(date):\n",
        "  for key, val in sorted_ltz.items():\n",
        "    if date >= key:\n",
        "      last = val\n",
        "      continue\n",
        "    return last"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0IQ5mkTyTBt",
        "colab_type": "text"
      },
      "source": [
        "# Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8rRxxjwEm-M",
        "colab_type": "code",
        "outputId": "bbf0a185-7b9d-4c06-e43b-9979fcd83758",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "dataset = pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/saschaschworm/big-data-and-data-science/master/datasets/prediction-challenge/dataset.csv', \n",
        "    index_col='identifier', parse_dates=['date'])\n",
        "\n",
        "dataset.insert(len(dataset.columns) -1, \"weekday\", dataset.date.dt.weekday)\n",
        "dataset.insert(len(dataset.columns) -1, \"day\", dataset.date.dt.day)\n",
        "dataset.insert(len(dataset.columns) -1, \"month\", dataset.date.dt.month)\n",
        "dataset.insert(len(dataset.columns) -1, \"year\", dataset.date.dt.year)\n",
        "dataset.insert(len(dataset.columns) -1, \"quarter\", dataset.date.dt.quarter)\n",
        "\n",
        "# deleted cci (not useful)\n",
        "dataset.insert(len(dataset.columns)-1, \"leitzins\", dataset['date'].apply(get_leitzins))\n",
        "dataset.insert(len(dataset.columns)-1, \"euribor\", dataset['date'].apply(get_euribor))\n",
        "dataset.insert(len(dataset.columns)-1, \"cpi\", dataset['date'].apply(get_cpi))\n",
        "dataset.insert(len(dataset.columns)-1, \"fsi\", dataset['date'].apply(get_fsi))\n",
        "dataset.insert(len(dataset.columns)-1, \"eurostoxx\", dataset['date'].apply(get_eurostoxx))\n",
        "\n",
        "dataset = dataset.drop('date', axis=1)\n",
        "\n",
        "dataset.loc[dataset['days_since_last_contact'] == -1, 'days_since_last_contact'] = 10000\n",
        "\n",
        "dataset.columns"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['age', 'marital_status', 'education', 'job', 'credit_default',\n",
              "       'housing_loan', 'personal_loan', 'communication_type',\n",
              "       'n_contacts_campaign', 'days_since_last_contact', 'n_contacts_before',\n",
              "       'previous_conversion', 'duration', 'weekday', 'day', 'month', 'year',\n",
              "       'quarter', 'leitzins', 'euribor', 'cpi', 'fsi', 'eurostoxx', 'success'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chldv7-yEP04",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_features = ['day' ,'quarter', 'education', 'job', 'age', \n",
        "             'previous_conversion', 'n_contacts_before',\n",
        "             'days_since_last_contact', 'n_contacts_campaign',\n",
        "             'marital_status', 'credit_default', 'duration',\n",
        "             'housing_loan', 'personal_loan', 'leitzins',\n",
        "             'eurostoxx', 'fsi', 'cpi']\n",
        "\n",
        "categorical_features = ['education', 'job', 'previous_conversion',\n",
        "                        'marital_status', 'quarter', 'credit_default',\n",
        "                        'personal_loan', 'housing_loan']\n",
        "\n",
        "numerical_features = ['n_contacts_campaign', 'duration', \n",
        "                  'days_since_last_contact', 'age', 'cpi',\n",
        "                  'leitzins', 'eurostoxx', 'fsi', 'day']\n",
        "\n",
        "X = dataset[all_features]\n",
        "y = dataset['success']\n",
        "y = y.apply(lambda x: 1 if x == \"Yes\" else 0)\n",
        "\n",
        "for item in categorical_features:\n",
        "  try:\n",
        "    encoded = pd.get_dummies(X[item], prefix=item)\n",
        "    X.drop(item, axis=1, inplace=True)\n",
        "    X = X.join(encoded)\n",
        "  except Exception as e:\n",
        "    print(\"Something went wrong?!\")\n",
        "    print(e)\n",
        "    continue\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X[numerical_features] = scaler.fit_transform(X[numerical_features])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFlNcu4m2foR",
        "colab_type": "text"
      },
      "source": [
        "# Model Evaluation\n",
        "\n",
        "The target variable of this problem is bianary (Yes/No). Therefore, we the model needs to be able to handle binary classification. The following models were selected for preselection:\n",
        "## Logistic Regression\n",
        "A regression (linear) model outputs a continuous target variable which is transformed to a binary using a decision boundary.\n",
        "## Decision Tree (Random Forest)\n",
        "A random forest is (non-linear) model which constructs multiple decision trees which bisect the datasets into different classes by generating multiple decision boundaries.\n",
        "## Extreme Gradiant Boosting Classification (XGBClassifier)\n",
        "The XGBClassifier is a random forest which uses gradiant optimization to construct the decision trees (weak learners)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuU0JXcs2faK",
        "colab_type": "code",
        "outputId": "47df47ca-f541-4296-d1c4-1741ab2ce697",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "custom_scorer = make_scorer(f1_score, pos_label=1)\n",
        "\n",
        "# The evaluation is done by running all models with the same preprocessed datasets\n",
        "# Every model will use their default hyperparameters. No optimization has been conducted\n",
        "# The evaluation is done with the F1 Score for all models using \n",
        "# a k-fold cross-validation (RepeatedStratifiedKFold)\n",
        "\n",
        "# function which takes a model and the training data to run a \n",
        "# cross_validate with the f1-scorer\n",
        "def validate_model(model, X, y):\n",
        "  scoring=custom_scorer\n",
        "  cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, \n",
        "                               random_state=1909)\n",
        "  scores = cross_validate(model, X, y, scoring=scoring, cv=cv, \n",
        "                          n_jobs=-1, return_train_score=True)\n",
        "  train_score = np.mean(scores[\"train_score\"])*100\n",
        "  test_score = np.mean(scores[\"test_score\"])*100\n",
        "  print(f'Mean F1 Score of {str(model).split(\"(\")[0]}: {train_score:.2f}/{test_score:.2f}\\n')\n",
        "  return {\n",
        "      'train': train_score,\n",
        "      'test': test_score\n",
        "      }\n",
        "\n",
        "model = LogisticRegression() # default hyperparameters\n",
        "print(f'Evaluating Model: {str(model).split(\"(\")[0]}')\n",
        "lgr_score = validate_model(model, X, y)\n",
        "\n",
        "model = RandomForestClassifier() # else: default hyperparameters\n",
        "print(model)\n",
        "print(f'Evaluating Model: {str(model).split(\"(\")[0]}')\n",
        "rfc_score = validate_model(model, X, y)\n",
        "\n",
        "model = XGBClassifier() # else: default hyperparameters\n",
        "print(model)\n",
        "print(f'Evaluating Model: {str(model).split(\"(\")[0]}')\n",
        "xgbc_score = validate_model(model, X, y)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluating Model: LogisticRegression\n",
            "Mean F1 Score of LogisticRegression: 47.77/47.53\n",
            "\n",
            "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
            "                       criterion='gini', max_depth=None, max_features='auto',\n",
            "                       max_leaf_nodes=None, max_samples=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
            "                       n_jobs=None, oob_score=False, random_state=None,\n",
            "                       verbose=0, warm_start=False)\n",
            "Evaluating Model: RandomForestClassifier\n",
            "Mean F1 Score of RandomForestClassifier: 99.99/52.37\n",
            "\n",
            "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
            "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
            "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
            "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
            "              nthread=None, objective='binary:logistic', random_state=0,\n",
            "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
            "              silent=None, subsample=1, verbosity=1)\n",
            "Evaluating Model: XGBClassifier\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mean F1 Score of XGBClassifier: 58.06/56.20\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P7d4DFsXiyB",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameter optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Qi7nwcigeyl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "custom_scorer = make_scorer(f1_score, pos_label=1)\n",
        "\n",
        "def gridsearch(model, param_grid, X, y):\n",
        "  search = GridSearchCV(model, param_grid=param_grid, scoring=custom_scorer,\n",
        "                    n_jobs=-1, cv=4, refit=True, error_score=0).fit(X, y)\n",
        "\n",
        "  print(f'Optimal parameters: {search.best_params_}')\n",
        "  print(f'Best Score: {search.best_score_}')\n",
        "  return search.best_params_\n",
        "\n",
        "def randsearch(model, param_distr, X, y):\n",
        "  search = RandomizedSearchCV(model, param_distributions=param_distr, \n",
        "                              n_iter=10, scoring=custom_scorer, n_jobs=-1, \n",
        "                              cv=10, random_state=1909).fit(X, y)\n",
        "\n",
        "  print(f'Optimal parameters: {search.best_params_}')\n",
        "\n",
        "  print(f'Best Score: {search.best_score_}')\n",
        "  return search.best_params_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPKfRKY2I3OE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# basis hyperparams\n",
        "hyperparams = {\n",
        "    'scale_pos_weight': 9,\n",
        "    'booster': 'gbtree', \n",
        "    'objective': 'binary:logistic',\n",
        "    'silent': True, \n",
        "    'missing': None,\n",
        "    'nthread': -1,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgpktDV1hUpo",
        "colab_type": "code",
        "outputId": "8f51941c-5662-4b53-833d-fa28f046ef36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from scipy.stats import randint, uniform\n",
        "\n",
        "custom_scorer = make_scorer(f1_score, pos_label=1)\n",
        "\n",
        "n_estimators = randint(100, 150)\n",
        "max_depth = randint(10, 50)\n",
        "learning_rate = uniform(0.005, 0.3)\n",
        "colsample_bynode = uniform(0.25, 1)\n",
        "colsample_bylevel = uniform(0.25, 1)\n",
        "colsample_bytree = uniform(0.25, 1)\n",
        "subsample = uniform(0.25, 1)\n",
        "gamma = randint(0, 5)\n",
        "base_score = uniform(0.2, 0.8)\n",
        "min_child_weight = uniform(0.2, 1)\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': n_estimators,\n",
        "    'max_depth': max_depth,\n",
        "    'learning_rate': learning_rate,\n",
        "    'subsample': subsample,\n",
        "    'colsample_bynode': colsample_bynode,\n",
        "    'colsample_bylevel': colsample_bylevel,\n",
        "    'colsample_bytree': colsample_bytree,\n",
        "    'base_score': base_score,\n",
        "    'gamma': gamma,\n",
        "    'min_child_weight': min_child_weight,\n",
        "}\n",
        "\n",
        "best_params = randsearch(XGBClassifier(**hyperparams), \n",
        "                         param_distr=param_grid, X=X, y=y)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimal parameters: {'base_score': 0.30070711686028756, 'colsample_bylevel': 0.950693101530163, 'colsample_bynode': 0.40027174935474574, 'colsample_bytree': 0.7309167840661879, 'gamma': 0, 'learning_rate': 0.012986666228874295, 'max_depth': 49, 'min_child_weight': 0.4641502506772979, 'n_estimators': 140, 'subsample': 0.3930716248652387}\n",
            "Best Score: 0.636426809104316\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkwIIGQLlgM0",
        "colab_type": "code",
        "outputId": "06c7edb7-da86-43f2-8c46-750b73cf7f6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# with best parameters from gridsearch\n",
        "f_hyperparams = {**hyperparams, **{\n",
        "    'learning_rate': best_params['learning_rate'],\n",
        "    'n_estimators': best_params['n_estimators'],\n",
        "    'max_depth': best_params['max_depth'],\n",
        "    'colsample_bylevel': best_params['colsample_bylevel'],\n",
        "    'colsample_bynode': best_params['colsample_bynode'],\n",
        "    'colsample_bytree': best_params['colsample_bytree'],\n",
        "    'subsample': best_params['subsample'],\n",
        "    'gamma': best_params['gamma'],\n",
        "    'base_score': best_params['base_score'],\n",
        "    'min_child_weight': best_params['min_child_weight'],\n",
        "    }\n",
        "}\n",
        "\n",
        "model = XGBClassifier(**f_hyperparams)\n",
        "print(model)\n",
        "validate_model(model, X, y)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XGBClassifier(base_score=0.30070711686028756, booster='gbtree',\n",
            "              colsample_bylevel=0.950693101530163,\n",
            "              colsample_bynode=0.40027174935474574,\n",
            "              colsample_bytree=0.7309167840661879, gamma=0,\n",
            "              learning_rate=0.012986666228874295, max_delta_step=0,\n",
            "              max_depth=49, min_child_weight=0.4641502506772979, missing=None,\n",
            "              n_estimators=140, n_jobs=1, nthread=-1,\n",
            "              objective='binary:logistic', random_state=0, reg_alpha=0,\n",
            "              reg_lambda=1, scale_pos_weight=9, seed=None, silent=True,\n",
            "              subsample=0.3930716248652387, verbosity=1)\n",
            "Mean F1 Score of XGBClassifier: 75.61/63.76\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test': 63.75749847789783, 'train': 75.6090351291712}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzzGIcPscnZL",
        "colab_type": "text"
      },
      "source": [
        "# Evaluating Model with Optimized Parameter\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Bq8TKCUt_Zs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hyperparams = {'seed': 1909, \n",
        "               'learning_rate': 0.01, \n",
        "               'min_child_weight': 0.8, \n",
        "               'scale_pos_weight': 8, \n",
        "               'colsample_bylevel': 0.75, \n",
        "               'colsample_bytree': 0.65, \n",
        "               'colsample_bynode': 0.75, \n",
        "               'max_depth': 50, \n",
        "               'n_estimators': 100, \n",
        "               'gamma': 0.95, \n",
        "               'subsample': 0.25, \n",
        "               'base_score': 0.5, \n",
        "               'reg_lambda': 1, \n",
        "               'nthread': -1, 'booster': 'gbtree', \n",
        "               'objective': 'binary:logistic', 'silent': True, \n",
        "               'missing': None, 'max_delta_step': 0\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Yq3wayYwhLT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1909)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gC4O0MskwwZt",
        "colab_type": "code",
        "outputId": "89c6145a-fc62-4aa1-a53c-ef9b4dd46ce4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "model = XGBClassifier(**f_hyperparams)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_pred, y_test))\n",
        "print(confusion_matrix(y_pred, y_test))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.97      0.94      9335\n",
            "           1       0.77      0.54      0.64      1786\n",
            "\n",
            "    accuracy                           0.90     11121\n",
            "   macro avg       0.84      0.76      0.79     11121\n",
            "weighted avg       0.89      0.90      0.89     11121\n",
            "\n",
            "[[9043  292]\n",
            " [ 814  972]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM7mwuyO8O1u",
        "colab_type": "text"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43J6FNEL8OUf",
        "colab_type": "code",
        "outputId": "52444f44-c108-4bea-a6ac-c857b3b63bc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "prediction_dataset = pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/saschaschworm/big-data-and-data-science/master/datasets/prediction-challenge/prediction-dataset.csv', \n",
        "    index_col='identifier', parse_dates=['date'])\n",
        "\n",
        "prediction_dataset.insert(len(prediction_dataset.columns) -1, \"weekday\", prediction_dataset.date.dt.weekday)\n",
        "prediction_dataset.insert(len(prediction_dataset.columns) -1, \"day\", prediction_dataset.date.dt.day)\n",
        "prediction_dataset.insert(len(prediction_dataset.columns) -1, \"month\", prediction_dataset.date.dt.month)\n",
        "prediction_dataset.insert(len(prediction_dataset.columns) -1, \"year\", prediction_dataset.date.dt.year)\n",
        "prediction_dataset.insert(len(prediction_dataset.columns) -1, \"quarter\", prediction_dataset.date.dt.quarter)\n",
        "\n",
        "prediction_dataset.insert(len(prediction_dataset.columns)-1, \"leitzins\", prediction_dataset['date'].apply(get_leitzins))\n",
        "prediction_dataset.insert(len(prediction_dataset.columns)-1, \"euribor\", prediction_dataset['date'].apply(get_euribor))\n",
        "prediction_dataset.insert(len(prediction_dataset.columns)-1, \"cpi\", prediction_dataset['date'].apply(get_cpi))\n",
        "prediction_dataset.insert(len(prediction_dataset.columns)-1, \"fsi\", prediction_dataset['date'].apply(get_fsi))\n",
        "prediction_dataset.insert(len(prediction_dataset.columns)-1, \"eurostoxx\", prediction_dataset['date'].apply(get_eurostoxx))\n",
        "\n",
        "prediction_dataset = prediction_dataset.drop('date', axis=1)\n",
        "prediction_dataset.columns"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['age', 'marital_status', 'education', 'job', 'credit_default',\n",
              "       'housing_loan', 'personal_loan', 'communication_type',\n",
              "       'n_contacts_campaign', 'days_since_last_contact', 'n_contacts_before',\n",
              "       'previous_conversion', 'weekday', 'day', 'month', 'year', 'quarter',\n",
              "       'leitzins', 'euribor', 'cpi', 'fsi', 'eurostoxx', 'duration'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiUOM5Im8MiO",
        "colab_type": "code",
        "outputId": "70678303-9ba5-426a-b71d-98ce8e86bc9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_pred = prediction_dataset[all_features]\n",
        "\n",
        "for item in categorical_features:\n",
        "  try:\n",
        "    encoded = pd.get_dummies(X_pred[item], prefix=item)\n",
        "    X_pred.drop(item, axis=1, inplace=True)\n",
        "    X_pred = X_pred.join(encoded)\n",
        "  except Exception as e:\n",
        "    print(\"Something went wrong?!\")\n",
        "    print(e)\n",
        "    continue\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_pred[numerical_features] = scaler.fit_transform(X_pred[numerical_features])\n",
        "\n",
        "# uff... Add missing features to the prediction dataset at the position it has\n",
        "# in the training dataset\n",
        "for feature in X.columns:\n",
        "  if not feature in X_pred.columns:\n",
        "    print(f' X_pred is missing {feature}')\n",
        "    X_pred.insert(X.columns.tolist().index(feature), feature, 0)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " X_pred is missing credit_default_Yes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsSPk1JUJzOp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1019c8e4-f64c-4159-e3c3-1d76b036e06b"
      },
      "source": [
        "mountpath = \"/content/drive\"\n",
        "from google.colab import drive\n",
        "drive.mount(mountpath)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzbQ05JN8KOx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = XGBClassifier(**f_hyperparams)\n",
        "model.fit(X, y)\n",
        "predictions = model.predict(X_pred)\n",
        "\n",
        "submission = pd.DataFrame(\n",
        "    predictions, index=X_pred.index, columns=['prediction'])\n",
        "\n",
        "matriculation_number = '465527'\n",
        "\n",
        "submission.to_csv(\n",
        "    f'{mountpath}/My Drive/seminararbeit/result/submission-{matriculation_number}.csv', index_label='identifier')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}